<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep learning | Nguyen Huy Thanh</title>
    <link>https://nhthanh0809.github.io/mybiography/tag/deep-learning/</link>
      <atom:link href="https://nhthanh0809.github.io/mybiography/tag/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>&amp;copy 2023 | Nguyen Huy Thanh</copyright>
    <image>
      <url>https://nhthanh0809.github.io/mybiography/images/icon_huba8e7a8e3138b570f03a1a1bd06356a6_10104_512x512_fill_lanczos_center_3.png</url>
      <title>Deep learning</title>
      <link>https://nhthanh0809.github.io/mybiography/tag/deep-learning/</link>
    </image>
    
    <item>
      <title>Automatic Cephalometric landmark detection</title>
      <link>https://nhthanh0809.github.io/mybiography/project/ceph_landmark_det/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nhthanh0809.github.io/mybiography/project/ceph_landmark_det/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Landmark identification&lt;/strong&gt; is crucial in quantifying cephalometric analysis such as Steiner, Bjork, Ricketts, Kim, Nagasaki, etc. For applying these analysis to cephalometric image, dentist or orthodontist need to annotate
some group of landmarks that corresponding to specified analysis. Manual annotation of landmarks is a tedious, laborious task and prone to human errors.
Therefore, it&amp;rsquo;s necessary that they need an efficient automated application for landmark identification. &lt;br&gt;
In this project, we developed an online efficient AI driven tools for automatically detecting landmark on Cephalometry image.
The core part of these tools is our developed deep learning model with backbone &lt;em&gt;&lt;strong&gt;DenseNet121&lt;/strong&gt;&lt;/em&gt;. This model was trained on both public dataset of Cephalometric image 
&lt;a href=&#34;https://biomedicalimaging.org/2015/program/isbi-challenges/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ISBI 2015&lt;/a&gt; and private dataset. &lt;br&gt;
These tools currently has been integrated with other advanced technology tools in 
&lt;a href=&#34;https://www.viceph.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Viceph&lt;/a&gt; - an online AI driven application for orthodontist.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fully automated Cervical Vertebral Maturation assessment</title>
      <link>https://nhthanh0809.github.io/mybiography/project/cvm_classification/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nhthanh0809.github.io/mybiography/project/cvm_classification/</guid>
      <description>&lt;p&gt;Cervical vertebra maturation (CVM) staging in lateral cephalometric radiographs is an efficient method to determine skeletal maturation,
as lateral cephalometric radiography is routinely required for orthodontic diagnosis and treatment planning in orthodontic practice with no additional radiographs required to assess the CVM stages.
In this researching theme, we researched and developed an application applied AI algorithms that were trained on
public dataset of Cephalometric image 
&lt;a href=&#34;https://biomedicalimaging.org/2015/program/isbi-challenges/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ISBI 2015&lt;/a&gt; for classifying CVM stages. &lt;br&gt;
We firstly annotated CVM landmarks and CVM class label for 400 images in 
&lt;a href=&#34;https://biomedicalimaging.org/2015/program/isbi-challenges/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ISBI 2015&lt;/a&gt;.
The annotated data were reviewed by 4 doctors (2 junior and 2 senior) to ensure reliability.
Subsequently, the landmarks data first was trained with deep learning landmark detection model. The model achieved 99.42% 
&lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/25794388/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Success Detection Rate (SDR)&lt;/a&gt; with respect to the 2mm precision range. &lt;br&gt;
After getting output from landmark detection model, we calculated our proposed cervical stage score values (the Figure) following to this 
&lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/29337631/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;User&amp;rsquo; Guide&lt;/a&gt;.
These score values closely follow to CVM assessment features that were introduced by 
&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S1073874605000216&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Baccetti et al.&lt;/a&gt;.
Finally, these calculated score values and annotated CVM class labels were treated as input tabular data for training different classification model such as Multi-Layer Perceptron (MLP), CatBoost, XGBoost, and LightGBM.
These models will output 6 CVM stages from CS1 to CS6 as final results. &lt;br&gt;
NOTE: These tools currently has been integrated with other advanced technology tools in 
&lt;a href=&#34;https://www.viceph.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Viceph&lt;/a&gt; - an online AI driven application for orthodontist.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Room layout recognition</title>
      <link>https://nhthanh0809.github.io/mybiography/project/room_layout_recognition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nhthanh0809.github.io/mybiography/project/room_layout_recognition/</guid>
      <description>&lt;p&gt;In this project, we focus on building-up an &lt;strong&gt;Instance segmentation&lt;/strong&gt; model for recognizing drawing room layout data.
We trained and tested our model (Unet architecture) on both public dataset (Cubicasa, R3D) and own dataset (more than 1000 images of drawing layout)
To ensuring generalization of the trained model, we added CVF-FP dataset (public) to test set for evaluation. &lt;br&gt;
After AI model development, we deployed the model on Torchserve - a Pytorch model serving framework, and integrated to Kubenetes system. The AI model
will be used for classifying room type, providing essential information for construction site works.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Synthesizing data by using 3D object models</title>
      <link>https://nhthanh0809.github.io/mybiography/project/synthesizing_360degree_data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nhthanh0809.github.io/mybiography/project/synthesizing_360degree_data/</guid>
      <description>&lt;p&gt;In this project, we focus on synthesizing 360 panoramic image data from 3D models without real image.
We first collected 3D models of in-house objects such as table, curtain, chairs, air-conditioner, TV, etc from 
&lt;a href=&#34;https://3dwarehouse.sketchup.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;online storage&lt;/a&gt;.
We then rendered these 3D model on a license software for visualizing 3D models.
We chose this software because it has a function that allowing rendering objects under virtual 360 degrees camera. Using this software, we generated virtual multi-view video and images of collected 3D objects.
Next, the videos and images will be processed by own developed tool allowing segment objects from background images.
This tool synthesized 360 panoramic image data by putting segmented object images on different background. &lt;br&gt;
Finally, we trained some well-known object detectors (YoloV3, YoloV5, EfficientDet, Mask-RCNN, etc.) on the synthesized data then evaluated on 360 degree public dataset namely 
&lt;a href=&#34;https://openaccess.thecvf.com/content_WACV_2020/html/Chou_360-Indoor_Towards_Learning_Real-World_Objects_in_360deg_Indoor_Equirectangular_Images_WACV_2020_paper.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;360-Indoor&lt;/a&gt;
The evaluation results show that this method is very efficient, low-cost and promising approach for synthesizing 360 image dataset.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Video object tracking and segmentation for digital camera</title>
      <link>https://nhthanh0809.github.io/mybiography/project/video_object_tracking_and_segmentation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://nhthanh0809.github.io/mybiography/project/video_object_tracking_and_segmentation/</guid>
      <description>&lt;p&gt;In this project, we focus on optimizing &lt;strong&gt;video object tracking and segmentation&lt;/strong&gt; model for running on digital still camera.
We used the Siamese network based model for tracking object on input video. The model was trained on larger video dataset such as Youtube-VOS, ImageNet-VID 2015, etc. &lt;br&gt;
For optimizing the model, firstly we change backbone of the model from ResNet50 to MobileNetV1. We then re-trained and re-evaluated the modified model on above datasets.
The evaluation show that while accuracy of the new model slightly decrease but performance significantly increase in comparison with the original one. &lt;br&gt;
Next step, we implemented quantization and pruning methods for compressing the model weight from Float 16 to Int 8, in order to it can run on edge device like digital camera.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
